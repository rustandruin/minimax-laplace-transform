\documentclass[xcolor={svgnames,pdftex,dvipsnames,table},10pt]{beamer} %was usenames
\usefonttheme[]{serif} 
\usefonttheme{professionalfonts}
\usecolortheme[named=MidnightBlue]{structure}
\usetheme[height=7mm]{Rochester}
\setbeamertemplate{blocks}[rounded][shadow=true]
\useoutertheme{umbcfootline}
\useinnertheme{umbctribullets}
\useinnertheme{umbcboxes}
\setfootline{\insertshortauthor \quad \insertshorttitle \quad \insertshortdate \hfill \insertframenumber/\inserttotalframenumber}
\usepackage[utf8]{inputenc}
\usepackage{kerkis}
\usepackage{bm}
\usepackage{colortbl}
%\usepackage[scaled=0.875]{helvet}%
\renewcommand{\ttdefault}{lmtt}%

%add footnotes to indicate support
\usepackage[absolute,overlay]{textpos} 
\newenvironment{support}[2]{% 
  \begin{textblock*}{\textwidth}(#1,#2) 
      \footnotesize\it\bgroup\color{black!50}}{\egroup\end{textblock*}}
			
\usepackage{tikz}
\usetikzlibrary{arrows,shapes,fit}
\tikzstyle{every picture}+=[remember picture]
\tikzstyle{na} = [baseline=-.5ex]
\everymath{\displaystyle}
\pgfdeclarelayer{background}
\pgfsetlayers{background,main}

\hypersetup{%
  pdftitle={Covariance estimation},%
  pdfauthor={Alex Gittens},%
  pdfsubject={SPARS 2011 workshop presentation},%
  pdfkeywords={random matrix theory, covariance estimation, tail bounds}%
}
\hypersetup{plainpages    = false,
            pdfnewwindow  = false}
            %pdfpagemode   = FullScreen}
\hypersetup{colorlinks=true,urlcolor=blue}

% add bookmarks for easier navigations
\usepackage{bookmark}
\usepackage{etoolbox}
\makeatletter
\apptocmd{\beamer@@frametitle}{
  % keep this line to add the frame title to the TOC at the "subsection level"
  \addtocontents{toc}{\protect\beamer@subsectionintoc{\the\c@section}{0}{#1}{\the\c@page}{\the\c@part}%
        {\the\beamer@tocsectionnumber}}%
  % keep this line to add a bookmark that shows up in the PDF TOC at the subsection level
  \bookmark[page=\the\c@page,level=3]{#1}
  }%
  {\message{** patching of \string\beamer@@frametitle succeeded **}}%
  {\message{** patching of \string\beamer@@frametitle failed **}}%
\makeatother

%\setbeamertemplate{navigation symbols}{}
\setbeamercovered{dynamic}


\newcommand{\mat}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\Prob}[1]{\ensuremath{\mathbb{P}\left\{#1 \right\}}}
\renewcommand{\star}{*}

\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\C}{\ensuremath{\mathbb{C}}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\e}{\ensuremath{\mathrm{e}}}

\DeclareMathOperator{\tr}{tr}

\newtheorem*{thm}{Theorem}

% Titlepage info
\title{Covariance estimation}
\subtitle[]{via tail bounds for eigenvalues of sums of random matrices}
\author[A. Gittens \and J.~A. Tropp]{Alex Gittens \and Joel A. Tropp}
\institute[Caltech]{%
Department of Computing and Mathematical Sciences \\
California Institute of Technology \\
\href{mailto:gittens@caltech.edu}{gittens@caltech.edu} \\
\href{mailto:jtropp@cms.caltech.edu}{jtropp@cms.caltech.edu} 
}
%\date[SPARS '11]{Signal Processing with Adaptive Sparse Structured Representations 2011}
\date[SPARS '11]{SPARS Workshop 2011}

\begin{document}

\begin{frame}[plain]
\begin{support}{19mm}{85mm}
${}^\star$Research supported by ONR and AFOSR awards and a Sloan Fellowship.
\end{support}
\titlepage
\end{frame}

\begin{frame}{Problem Statement}

Let $\mat{x} \in \R^p$ be a zero-mean high-dimensional random vector. Information on the dependence structure of $\mat{x}$ is captured by the covariance matrix
\[
\mat{\Sigma} = \E \mat{x} \mat{x}^\star.
\]
The sample covariance matrix is a classical estimator for $\mat{\Sigma}:$
\[
\widehat{\mat{\Sigma}}_n = \frac{1}{n} \sum\nolimits_{i=1}^n \mat{x}_i\mat{x}_i^\star.
\]

\begin{displaybox}{0.7\linewidth}
\parbox{\linewidth}{How many samples of $\mat{x}$ are required so that $\widehat{\mat{\Sigma}}_n$ accurately estimates $\mat{\Sigma}?$}
\end{displaybox}

\end{frame}

\begin{frame}{What is known}

Typically accuracy is measured in spectral norm.
\vspace{1em}
\begin{displaybox}{0.7\linewidth}
\parbox{\linewidth}{%
How many samples ensure that
\[ \|\mat{\Sigma} - \widehat{\mat{\Sigma}}_n\|_2 \leq \varepsilon \|\mat{\Sigma}\|_2? \]
}
\end{displaybox}
\pause
\begin{itemize}
    \item for \textcolor{OliveGreen}{log-concave} distributions $\Omega(p)$ samples suffice (Adamczak et al. 2011), 
		\pause
    \item for distributions with \textcolor{OliveGreen}{finite fourth moments}, $\tilde{\Omega}(p)$ samples suffice (Vershynin 2011a), 
		\pause
		\item for distributions with \textcolor{OliveGreen}{finite $2+\varepsilon$ moments} that satisfy a regularity condition, $\Omega(p)$ samples suffice (Vershynin 2011b),
		\pause
    \item for distributions with \textcolor{OliveGreen}{finite second moments}, $\Omega(p\log p)$ samples suffice (Rudelson 1999). 
\end{itemize}
\end{frame}

\begin{frame}{An observation}
A relative spectral error bound,
\[
\|\mat{\Sigma} - \widehat{\mat{\Sigma}}_n\|_2 \leq \varepsilon \|\mat{\Sigma}\|_2,
\]
ensures recovery of the top eigenpair of $\mat{\Sigma},$ \ldots

\vspace{1em}
\pause
but does \emph{not} ensure the recovery of the remaining eigenpairs:
\[
|\lambda_k(\mat{\Sigma}) - \lambda_k(\widehat{\mat{\Sigma}}_n)| < \varepsilon \|\mat{\Sigma}\|_2 
\]
is not meaningful if $\lambda_k \ll \lambda_1.$

\vspace{1em}
\pause
Using known relative spectral error bounds, need O$(\varepsilon^{-2} \kappa(\mat{\Sigma}_\ell)^2 p)$ measurements to get relative error recovery of the top $\ell$ eigenvalues.
\end{frame}

\begin{frame}{\dots and a question}

Maybe $\mat{\Sigma}$ has a decaying spectrum.
What if we want accurate estimates of a few of its eigenvalues?
\vspace{1em}
\begin{displaybox}{0.7\textwidth}
\parbox{\textwidth}{%
How many samples ensure the top $\ell \ll p$ eigenvalues are estimated to relative accuracy,
\[
|\lambda_k(\mat{\Sigma}) - \lambda_k(\widehat{\mat{\Sigma}}_n)| \leq \varepsilon \lambda_k(\mat{\Sigma})?
\]
}
\end{displaybox}

\vspace{1em}
Do we really need O($p$) measurements to recover just a few of the top eigenvalues?

\end{frame}

\begin{frame}{A simplified result}

\begin{thm}
Let the samples be drawn from a $\mathcal{N}(\mat{0}, \mat{\Sigma})$ distribution. Assume $\lambda_k$ decays sufficiently for $k > \ell$. If $\varepsilon \in (0, 1]$ and 
\[
n = \Omega(\varepsilon^{-2} \kappa(\mat{\Sigma}_\ell)^2 \ell \log p),
\]
then with high probability, for each $k=1,\ldots,\ell,$
\[
|\lambda_k(\widehat{\mat{\Sigma}}_n) - \lambda_k(\mat{\Sigma})| \leq \varepsilon \lambda_k(\mat{\Sigma}) 
\]
\end{thm}

\begin{itemize}
\pause
	\item Sufficient decay is, (other conditions give other results)
	\[ \sum_{i > \ell} \lambda_i/\lambda_1 \leq C.
	\] 
	This is satisfied if, e.g., the tail eigenvalues, $k > \ell,$ correspond to spread-spectrum noise or decay like $\tfrac{1}{i^{(1+\iota)}
	}$ for some $\iota > 0.$
	\pause
	\item The approach generalizes to other subgaussian distributions.
\end{itemize}

\end{frame}

\begin{frame}{More generally}
Restrict, for each $k$, probability that $\hat{\lambda}_k$ under/overestimates $\lambda_k.$
\begin{itemize}
\pause 
\item an upper bound on $\lambda_k$ 
\[
n = \frac{8}{3 \varepsilon^2} \tikz[baseline] \node[anchor=base,rounded corners,fill=OliveGreen!30] {$\kappa(\mat{\Sigma}_k) \frac{\tr{\mat{\Sigma}_k}}{\lambda_k}(\log k + \beta \log p)$}; \Rightarrow \Prob{\frac{\hat{\lambda}_k}{1-\epsilon} > \lambda_k} > 1- p^{-\beta}
\]

\pause
	\item a lower bound on $\lambda_k$
\begin{multline*}
n = \frac{1}{32\varepsilon^2} 
\tikz[baseline] \node[anchor=base,rounded corners,fill=BurntOrange!30] {$\frac{\big(\sum\nolimits_{i \geq k}\lambda_i \big)}{\lambda_k} \textstyle (\log (p-k+1) + \beta \log p)$}; \\
\Rightarrow \Prob{\frac{\hat{\lambda}_k}{1+\varepsilon} < \lambda_k} > 1-p^{-\beta}.
\end{multline*}

\pause 

\item Assuming decay
\begin{center}
	\begin{tabular}{l >{\columncolor{OliveGreen!30}}c >{\columncolor{BurntOrange!30}}c}
	\multicolumn{1}{l}{} & \multicolumn{1}{l}{upper bound} & \multicolumn{1}{l}{lower bound} \\ 
		$\lambda_1$ & O$(\log p)$ & O$(\ell \log p)$ \\
		$\lambda_\ell$ & O$(\kappa^2(\mat{\Sigma}_\ell) \ell \log p)$ & O$(\kappa(\mat{\Sigma}_\ell) \log p)$ 
	\end{tabular}
\end{center}

%\pause
	%\item a lower bound on $\lambda_k$
%
%\[
	%n = \frac{8}{3\varepsilon^2} 
	%\begin{tikzpicture}[baseline]
	%\node[anchor=base] (n2) 
	%{$\kappa(\mat{\Sigma}) \frac{\tr{\mat{\Sigma}}}{\lambda_p} (\log p + \delta)$}; 
		%
%\begin{pgfonlayer}{background}
%\node[fill=BurntOrange!40,draw=BurntOrange,fit=(n2)] {};
%\end{pgfonlayer}
%\end{tikzpicture}
	%\Rightarrow \Prob{\hat{\lambda}_p \leq (1-\varepsilon)\lambda_p} \leq \e^{-\delta}.
%\]
	%\begin{itemize}
	 %\item boo
	%\end{itemize}
	%
\end{itemize}

%\pause 
%If the spectrum decays:
%\begin{itemize}
 %\item Can be as small as $\log^2 p$  \tikz[na] \node[coordinate] (l1) {};
%\begin{tikzpicture}[overlay]
 %\path[->,OliveGreen,thick] (l1) edge [out=90, in=-90] (n1);
%\end{tikzpicture}
%
%\pause
%
 %\item Can be as small as $\kappa(\mat{\Sigma})^2 \log^2 p$  \tikz[na] \node[coordinate] (l2) {};
%\begin{tikzpicture}[overlay]
 %\path[->,BurntOrange,thick] (l2) edge [out=0, in=-90] (n2);
%\end{tikzpicture}
%\end{itemize}
%
\end{frame}

\begin{frame}{Proof sketch}

It suffices to show
\[
 \textstyle \Prob{\hat{\lambda}_k \geq (1+\varepsilon) \lambda_k } \quad \text{ and } \quad \Prob{\hat{\lambda}_k \leq (1-\varepsilon) \lambda_k}
\]
decay like $\mathrm{C} \exp(-\mathrm{c} n \epsilon^2)$ when $\epsilon$ is sufficiently small.

\pause
\begin{enumerate}
		
	\item Reduce the probability of each case occuring to the probability that the norm of an appropriate matrix is large.

	\pause
	
	\item Use matrix Bernstein bounds to establish the correct decay of these norms. 

	\pause
	
	\item Take a union bound over the indices $k.$ 
 \end{enumerate}
 
\end{frame}

\begin{frame}{Reduction for $\hat{\lambda}_k \geq \lambda_k + t$}

Let $\mat{B}$ have orthonormal columns and span the bottom $(p-k+1)$-dimensional invariant subspace of $\mat{\Sigma}.$

\underline{Claim}
\[
\Prob{
\begin{tikzpicture}[baseline]
\node[anchor=base] (esteigenval) {$\hat{\lambda}_k$};
%\onslide<3->{\begin{pgfonlayer}{background}
%\node[fill=OliveGreen!40,draw=OliveGreen,fit=(esteigenval)] {};
%\end{pgfonlayer}}
\end{tikzpicture}
\geq 
\tikz[baseline]{\node[anchor=base] (eigenval) {$\lambda_k$};} + t 
} \leq
\Prob{
\tikz[baseline]{\node[anchor=base] (pinchedesteigenval) {$\lambda_1(\mat{B}^\star \widehat{\mat{\Sigma}}_n \mat{B})$}; }
\geq  
\tikz[baseline]{\node[anchor=base] (eigenvalexpansion) {$\lambda_1(\mat{B}^\star \mat{\Sigma} \mat{B})$};} + t }.
\]

\pause
\emph{Proof.}

By Courant--Fischer,
\[
 \lambda_k(\mat{\Sigma}) = \lambda_1(\mat{B}^\star \mat{\Sigma} \mat{B})
\]

\pause
and
\[
\lambda_k(\widehat{\mat{\Sigma}}_n) = \min_{\substack{ \mat{V} \in \C^{p \times (p-k+1)} \\ \mat{V}^\star\mat{V}=\mat{I}}} \lambda_1(\mat{V}^\star \widehat{\mat{\Sigma}}_n\mat{V}) \leq \lambda_1(\mat{B}^\star \widehat{\mat{\Sigma}}_n \mat{B}).
\]

\qed
\end{frame}

\begin{frame}{Using the reduction}
Need to control RHS of 
\[
\Prob{\hat{\lambda}_k \geq \lambda_k + t }
\leq
\Prob{
\lambda_1(\mat{B}^\star \widehat{\mat{\Sigma}}_n \mat{B}) \geq  
\lambda_1(\mat{B}^\star \mat{\Sigma} \mat{B}) + t 
}
\]
\pause
Note:
\begin{itemize}
	\item 
$\lambda_1(\mat{B}^\star \hat{\mat{\Sigma}}_n \mat{B}) \rightarrow \lambda_1(\mat{B}^\star \mat{\Sigma} \mat{B}),$ and
	\item $\mat{B}^\star \hat{\mat{\Sigma}}_n \mat{B} = \sum\nolimits_i \mat{B}^\star \mat{x}_i\mat{x}_i^\star \mat{B}$ is a sum of independent random matrices.
\end{itemize}
\pause
Use estimates of the matrix moments of the summands to quantify the convergence. 
\begin{itemize}
\pause
\item If $\mat{g} \sim \mathcal{N}(\mat{0}, \mat{C})$, then for $m \geq 2,$
\[
\E(\mat{g}\mat{g}^\star)^m \preceq 2^m m!\,(\tr{\mat{C}})^{m-1} \cdot \mat{C}.
\]
\item Other subgaussian distributions satisfy similar relations. Can also substitute bounds on matrix moment generating functions, 
\[
\E \exp\left(\theta \mat{y}\mat{y}^\star \right) \preceq \mat{U}(\theta).
\]
\end{itemize}

\end{frame}

%\begin{frame}{Bernstein inequality}
 %Consider the probability
%\begin{multline*}
 %\Prob{\lambda_1(\mat{B}^\star \widehat{\mat{\Sigma}}_n \mat{B}) \geq  \lambda_1(\mat{B}^\star \mat{\Sigma} \mat{B}) + t } \\
 %= \Prob{\lambda_1\left(\sum_{i=1}^n \mat{B}^\star \mat{x}_i \mat{x}_i^\star \mat{B} \right) \geq n \lambda_1(\mat{B}^\star \mat{\Sigma} \mat{B}) + n t}.
%\end{multline*}
%
%The real variable analogue,
%\[
 %\Prob{ \sum_{i=1}^n x_i \geq t },
%\]
%can be bounded, using Bernstein's inequality, in terms of the variances of the summands and a uniform bound on their magnitudes.
%\end{frame}

\begin{frame}{Matrix Bernstein inequality }
 We use a moment-based matrix analog of Bernstein's inequality.

\begin{thm}[Matrix Moment-Bernstein Inequality]
Suppose self-adjoint matrices $\{\mat{G}_i\}$ have dimension $d$ and 
\[ \E(\mat{G}_i^m) \preceq \frac{m!}{2} A^{m-2} \cdot \mat{C}_i^2 \quad \text{ for } m =2,3,4,\ldots.
\]
Set
\[ \mu = \lambda_1\Big( \sum\nolimits_i \E\mat{G}_i \Big) \quad \text{and} \quad \sigma^2 = \lambda_1\Big(\sum\nolimits_i \mat{C}_i^2 \Big). \]
Then, for any $t \geq 0,$
\[	
 \Prob{ \lambda_1\Big(\sum\nolimits_i \mat{G}_i\Big) \geq \mu + t } \leq d \cdot \exp\Big(- \frac{t^2/2}{\sigma^2 + At} \Big).
\]
\end{thm}

\end{frame}

\begin{frame}{Finishing the argument}
 After computing $A$ and $\mat{C}_i^2$ for the summands $\mat{B}^\star \mat{x}_i \mat{x}_i^\star \mat{B},$ this gives
\[
 \Prob{\hat{\lambda}_k \geq \lambda_k +t} \leq (p-k+1)\cdot \exp\left( \frac{-nt^2}{32\lambda_k \sum_{i \geq k} \lambda_i } \right) \quad \text{ for } t \leq 4n \lambda_k.
\]
Finally, take $t = \varepsilon \lambda_k$ to see
\[
 \Prob{\hat{\lambda}_k \geq (1+\varepsilon) \lambda_k} \leq (p-k+1)\cdot \exp\left( \frac{-n\varepsilon^2}{32\sum_{i \geq k} \frac{\lambda_i}{\lambda_k} } \right) \quad \text{ for } \varepsilon \leq 4n.
\]
The proof for the case $\hat{\lambda}_k \leq \lambda_k - t$ is similar.
\qed
\end{frame}

\begin{frame}{Details}
``{\it Tail Bounds for All Eigenvalues of A Sum of Random Matrices}'', Gittens and Tropp, 2011. Preprint, \href{http://arxiv.org/abs/1104.4513}{arXiv:1104.4513}.
\begin{itemize}
\item Elaboration on the relative error estimation results.
\item Similar arguments to find tail bounds for all eigenvalues of a sum of \emph{arbitrary} random matrices.
\item An application to column subsampling.
\end{itemize}

\end{frame}
\end{document}

