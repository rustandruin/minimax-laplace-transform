\documentclass[xcolor={svgnames,pdftex,dvipsnames,table},10pt]{beamer} %was usenames
\usefonttheme[]{serif} 
\usefonttheme{professionalfonts}
\usecolortheme[named=MidnightBlue]{structure}
\usetheme[height=7mm]{Rochester}
\setbeamertemplate{blocks}[rounded][shadow=true]
\useoutertheme{umbcfootline}
\useinnertheme{umbctribullets}
\useinnertheme{umbcboxes}
\setfootline{\insertshortauthor \quad \insertshorttitle \quad \insertshortdate \hfill \insertframenumber/\inserttotalframenumber}
\usepackage[utf8]{inputenc}
\usepackage{kerkis}
%\usepackage[scaled=0.875]{helvet}%
\renewcommand{\ttdefault}{lmtt}%

\hypersetup{%
  pdftitle={Covariance estimation},%
  pdfauthor={Alex Gittens},%
  pdfsubject={SPARS 2011 workshop presentation},%
  pdfkeywords={random matrix theory, covariance estimation, tail bounds}%
}
\hypersetup{plainpages    = false,
            pdfnewwindow  = false}
            %pdfpagemode   = FullScreen}
\hypersetup{colorlinks=true,urlcolor=blue}


%\setbeamertemplate{navigation symbols}{}
\setbeamercovered{dynamic}


\newcommand{\mat}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\Prob}[1]{\ensuremath{\mathbb{P}\left\{#1 \right\}}}
\renewcommand{\star}{*}

\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\C}{\ensuremath{\mathbb{C}}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}

\newtheorem*{thm}{Theorem}

% Titlepage info
\title{Covariance estimation}
\subtitle[]{an application of tail bounds for sums of random matrices}
\author[A. Gittens \and J.~A. Tropp]{Alex Gittens \and Joel A. Tropp}
\institute[Caltech]{%
Department of Computing and Mathematical Sciences \\
California Institute of Technology \\
\href{mailto:gittens@caltech.edu}{gittens@caltech.edu} \\
\href{mailto:jtropp@cms.caltech.edu}{jtropp@cms.caltech.edu} 
}
%\date[SPARS '11]{Signal Processing with Adaptive Sparse Structured Representations 2011}
\date[SPARS '11]{SPARS Workshop 2011}

\begin{document}

\begin{frame}[plain]
\titlepage
\end{frame}

\begin{frame}{Problem Statement}

Let $\mat{x}$ be a centered high-dimensional random vector. First order information on the dependence structure of $\mat{x}$ is captured by the covariance matrix
\[
\mat{\Sigma} = \E \mat{x} \mat{x}^\star = (\E x_i x_j)_{i,j=1}^p = (\text{cov}(x_i, x_j))_{i=1}^p.
\]
The sample covariance matrix is a classical estimator for $\mat{\Sigma}:$
\[
\widehat{\mat{\Sigma}}_n = \frac{1}{n} \sum_{i=1}^n \mat{x}_i\mat{x}_i^\star.
\]

\begin{displaybox}{0.7\linewidth}
\parbox{\linewidth}{How many samples of $\mat{x}$ are required so that $\widehat{\mat{\Sigma}}_n$ accurately estimates $\mat{\Sigma}?$}
\end{displaybox}

\end{frame}

\begin{frame}{What is known}

Typically accuracy is measured in spectral norm.

\begin{displaybox}{0.7\linewidth}
\parbox{\linewidth}{%
How many samples ensure that
\[ \|\mat{\Sigma} - \widehat{\mat{\Sigma}}_n\|_2 \leq \varepsilon \|\mat{\Sigma}\|_2? \]
}
\end{displaybox}
 
\pause
\begin{itemize}
    \item for log-concave distributions $\Omega(p)$ samples suffice (Adamczak et al. 2011), 
		\pause
    \item for distributions with finite fourth moments, $\tilde{\Omega}(p)$ samples suffice (Vershynin 2011), 
		\pause
    \item for distributions with finite second moments $\Omega(p\log p)$ samples suffice (Rudelson 1999). 
\end{itemize}
\end{frame}

\begin{frame}{An observation}
A condition of the sort
\[
\|\mat{\Sigma} - \widehat{\mat{\Sigma}}_n\|_2 \leq \varepsilon \|\mat{\Sigma}\|_2
\]
is necessary for the recovery of eigenvectors of $\mat{\Sigma}$ \dots

but does \emph{not} ensure the recovery of any but the top eigenvalue:
\[
|\lambda_k(\mat{\Sigma}) - \lambda_k(\widehat{\mat{\Sigma}}_n)| < \varepsilon \|\mat{\Sigma}\|_2 
\]
is unsatisfactory if $\lambda_k \ll \lambda_1.$

Need O$(\varepsilon^{-2} \kappa(\mat{\Sigma})^2 p)$ measurements.
\end{frame}

\begin{frame}{\dots and a question}

What if we want accurate estimates of each eigenvalue, e.g. for rank estimation?

\begin{displaybox}{0.7\textwidth}
\parbox{\textwidth}{%
How many samples ensure the eigenvalues are estimated to relative accuracy,
\[
|\lambda_k(\mat{\Sigma}) - \lambda_k(\widehat{\mat{\Sigma}}_n)| \leq \varepsilon \lambda_k(\mat{\Sigma})?
\]
}
\end{displaybox}

Can we do better than $\epsilon^{-2} \kappa(\mat{\Sigma})^2 p$?

\end{frame}

\begin{frame}{A simplified result}

%\begin{tikzpicture}
%\node [theorembox] (box){%
%\begin{minipage}{\textwidth}
\begin{thm}
Let $\{\mat{x}_i\}_{i=1}^n \subset \R^p$ be i.i.d. samples drawn from a $\mathcal{N}(\mat{0}, \mat{\Sigma})$ distribution. 
If $\varepsilon \in (0, 1]$ and $n = \Omega_{\mat{\Sigma}}(\varepsilon^{-2} \log p),$ then with high probability
\[
|\lambda_k(\widehat{\mat{\Sigma}}_n) - \lambda_k(\mat{\Sigma})| \leq \varepsilon \lambda_k(\mat{\Sigma}) \quad \text{ for } k=1,\ldots,p.
\]
\end{thm}
%\end{minipage}
%};
%\node [fancytitle, right=10pt] at (box.north west) {Relative error eigenvalue estimation};
%\end{tikzpicture}
\begin{itemize}
	\item The constant in the estimate for $n$ depends on the spectrum of $\mat{\Sigma}$ and is between $\Omega(\kappa(\mat{\Sigma})^2)$ and $\Omega(\kappa(\mat{\Sigma})^2 p)$ .
	\item If $\mat{\Sigma}$ is rank $r,$ then $n= \Omega_{\mat{\Sigma}}(\varepsilon^{-2} \log r)$ samples suffice.
	\item The approach generalizes to other distributions.
\end{itemize}
\end{frame}

\begin{frame}{Proof sketch}

It suffices to show
\[
 \textstyle \Prob{\hat{\lambda}_k \geq (1+\varepsilon) \lambda_k } \quad \text{ and } \quad \Prob{\hat{\lambda}_k \leq (1-\varepsilon) \lambda_k}
\]
decay like $p\cdot \exp(-n\epsilon^2/O(p)).$

\pause
\begin{enumerate}
	\item Address the cases $\hat{\lambda}_k \leq \lambda_k -t $ and $\hat{\lambda}_k \geq \lambda_k + t$ separately. 
	
	\pause
	
	\item {\bf Reduce the probability of each case occuring to the probability that the norm of an appropriate matrix is large.}

	\pause
	
	\item {\bf Use matrix Bernstein bounds to establish the subexponential tails of this norm. }

	\pause
	
	\item Take a union bound over the indices $k.$
 \end{enumerate}
 
\end{frame}

\begin{frame}{Reduction for $\hat{\lambda}_k \geq \lambda_k + t$}

Let $\mat{B}$ have orthonormal columns and span the bottom $(p-k+1)$-dimensional invariant subspace of $\mat{\Sigma}.$

By Courant--Fischer,
\[
 \lambda_k(\mat{\Sigma}) = \lambda_1(\mat{B}^\star \mat{\Sigma} \mat{B})
\]
and
\[
\lambda_k(\widehat{\mat{\Sigma}}_n) = \min_{\substack{ \mat{V} \in \C^{p \times (p-k+1)} \\ \mat{V}^\star\mat{V}=\mat{I}}} \lambda_1(\mat{V}^\star \widehat{\mat{\Sigma}}_n\mat{V}) \leq \lambda_1(\mat{B}^\star \widehat{\mat{\Sigma}}_n \mat{B}).
\]
 
Thus 
\[
\Prob{\hat{\lambda}_k \geq \lambda_k + t } \leq \Prob{\lambda_1(\mat{B}^\star \widehat{\mat{\Sigma}}_n \mat{B}) \geq  \lambda_1(\mat{B}^\star \mat{\Sigma} \mat{B}) + t }.
\]

\end{frame}

\begin{frame}{Bernstein inequality}
 Consider the probability
\begin{multline*}
 \Prob{\lambda_1(\mat{B}^\star \widehat{\mat{\Sigma}}_n \mat{B}) \geq  \lambda_1(\mat{B}^\star \mat{\Sigma} \mat{B}) + t } \\
 = \Prob{\lambda_1\left(\sum_{i=1}^n \mat{B}^\star \mat{x}_i \mat{x}_i^\star \mat{B} \right) \geq n \lambda_1(\mat{B}^\star \mat{\Sigma} \mat{B}) + n t}.
\end{multline*}

The real variable analogue,
\[
 \Prob{ \sum_{i=1}^n x_i \geq t },
\]
can be bounded, using Bernstein's inequality, in terms of the variances of the summands and a uniform bound on their magnitudes.
\end{frame}

\begin{frame}{}
 We use a matrix version of Bernstein's inequality. Since the summands are not bounded, we substitute moment information.

\begin{thm}
Suppose matrices $\{\mat{X}_i\}$ have dimension $d$ and 
\[\textstyle \E(\mat{X}_i^m) \preceq \frac{m!}{2} M^{m-2} \mat{C}_i^2 \quad \text{ for } m =2,3,4,\ldots.
\]
Set
\[ \textstyle \mu = \lambda_1\left( \sum_i \E\mat{X}_i \right) \]
and define
\[ \textstyle \sigma^2 = \lambda_1\left(\sum_i \mat{C}_i^2 \right). \]
Then, for any $t \geq 0,$
\[	
 \textstyle \Prob{ \lambda_1\left(\sum_i \mat{X}_i\right) \geq \mu + t } \leq d \cdot \exp\left(- \frac{t^2/2}{\sigma^2 + Mt} \right).
\]
\end{thm}

\end{frame}

\begin{frame}{}
 After computing $M$ and $\mat{C}_i^2$ for the summands $\mat{B}^\star \mat{x}_i \mat{x}_i^\star \mat{B},$ this gives
\[
 \Prob{\hat{\lambda}_k \geq \lambda_k +t} \leq (p-k+1)\cdot \exp\left( \frac{-nt^2}{32\lambda_k \sum_{i \geq k} \lambda_i } \right) \quad \text{ for } t \leq 4n \lambda_k.
\]
Finally, take $t = \epsilon \lambda_k$ to see
\[
 \Prob{\hat{\lambda}_k \geq (1+\varepsilon) \lambda_k} \leq (p-k+1)\cdot \exp\left( \frac{-n\varepsilon^2}{32\sum_{i \geq k} \frac{\lambda_i}{\lambda_k} } \right) \quad \text{ for } \varepsilon \leq 4n.
\]
The proof for the case $\hat{\lambda}_k \leq \lambda_k - t$ is similar.
\qed
\end{frame}

\end{document}

